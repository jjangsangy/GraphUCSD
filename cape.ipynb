{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SDHacks 2015 Talk\n",
    "\n",
    "# Anatomy of Web Scrapers: Building Data Apps\n",
    "\n",
    "```\n",
    "$ whoami\n",
    "```\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"name\": \"Sang Han\",\n",
    "    \"work\": {\n",
    "         \"company\": \"Qadium\",\n",
    "        \"position\": \"Data Scientist\",\n",
    "     \"description\": \"Cybersecurity, Search and Machine Learning\"\n",
    "    }\n",
    "    \"school\": {\n",
    "           \"name\": \"University of California: San Diego\",\n",
    "        \"college\": \"Warren College\",\n",
    "           \"year\": 2011,\n",
    "          \"major\": [\"Physics\", \"Psycology\"]\n",
    "           \n",
    "    }\n",
    "    \"website\": \"http://sanghan.me\",\n",
    "     \"github\": \"https://github.com/jjangsangy\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [GraphUCSD](http://tabsoft.co/1B96Keb):\n",
    "\n",
    "#### Class Study App using UCSD CAPE Data\n",
    "Create a interactive visualization composed by the CAPE surveys filled at the end of each quarter\n",
    "__Code__: [Github](https://gist.github.com/jjangsangy/ef0d9b534c5f4ab58422)\n",
    "\n",
    "![graph_ucsd](http://i.imgur.com/DufDtIE.png)\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Installation\n",
    "\n",
    "Write stuff here\n",
    "\n",
    "Easy way (Only OS X and Linux)\n",
    "\n",
    "```\n",
    "$ make all\n",
    "```\n",
    "\n",
    "# Installation\n",
    "\n",
    "Install Python using Anacondas\n",
    "\n",
    "[Anacondas Python Distribution](http://continuum.io/downloads)\n",
    "\n",
    "Install Python Packages\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Important Libraries\n",
    "\n",
    "Basically just scraped cape website using Python (Both Python 2 and Python 3 Work), and I used PostgreSQL as the backend. Took about a day to write, and then another day just messing around to get everything to fit the schema, so it was a fun weekend project. The packages that are required to run the scraper are\n",
    "\n",
    "* [requests](https://github.com/kennethreitz/requests): for connecting to the site\n",
    "* [BeautifulSoup4](https://github.com/jjangsangy/BeautifulSoup4): for parsing\n",
    "* [pandas](http://pandas.pydata.org/):  for data mining\n",
    "* [SQLAlchemy](http://www.sqlalchemy.org/) as ORM.\n",
    "\n",
    "The [Anacondas Python Distribution](https://store.continuum.io/cshop/anaconda/) is like the easiest way to get all the packages needed if you wish to try out the code yourself.\n",
    "\n",
    "I also use a ThreadPool for making connections asynchronously, so that this doesn't take a million years lol.\n",
    "\n",
    "# Visualization\n",
    "\n",
    "The visualizations I used here are Tableau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Database\n",
    "\n",
    "So most of the code is actually data munging and cleaning up the data in order to fit the schema for PostgreSQL. \n",
    "\n",
    "Ultimately, the schema for Postgres looks like this.\n",
    "\n",
    "[Schema](http://i.imgur.com/JpMoiIz.jpg?1)\n",
    "\n",
    "This image is a little bit old, the new schema is a little different, but you get the idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A Note on Scraping\n",
    "\n",
    "I know that usually it's not polite to scrape from a service if they already provide an API, like reddit for instance. However, when I went to go look for one, I couldn't find any, so that gave me the green light to go ahead and write a scraper. And honestly, ever since I was a student (like 3 years ago), I was always unsatisfied with CAPE, so this is kind of my way of liberating the data so that students can access it better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Etc..\n",
    "\n",
    "Currently it only queries about 30-40 or so different departments and grabs the tables generated for those queries.\n",
    "\n",
    "However, every single class also has it's own page, but since I didn't want to make 20,000 HTTP requests, I went and only grabbed the front matter.\n",
    "\n",
    "This kind of opens it up for anyone else, or even myself to build a service that takes into account the rest of the data. In the scraper itself, I've created a column called `link` that actually points to the individual CAPE's for classes, so I've made it really easy for people to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import requests\n",
    "import sys\n",
    "import itertools\n",
    "import logging\n",
    "import string\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "from operator import itemgetter\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "\n",
    "try:\n",
    "    from urllib.parse import urljoin\n",
    "except ImportError:\n",
    "    from urlparse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def connect(prot='http', **q):\n",
    "    \"\"\"\n",
    "    Makes a connection with CAPE.\n",
    "    Required that at least one query is made.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    :params prot: Either HTTP or HTTPS\n",
    "    :params    q: Query Dictionary\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    :return: Request\n",
    "    :rtype : request.Request\n",
    "    \"\"\"\n",
    "    host   = 'cape.ucsd.edu'\n",
    "    inputs = 'Name', 'courseNumber', 'department'\n",
    "    prot   = prot.lower()\n",
    "    base   = '%s://%s/responses/Results.aspx' % (prot, host)\n",
    "\n",
    "    assert prot in ['http', 'https']\n",
    "    assert any(val in inputs for val in q)\n",
    "\n",
    "    headers = {           \"Host\": host,\n",
    "                        \"Accept\": ','.join([\n",
    "                                    \"text/html\",\n",
    "                                    \"application/xhtml+xml\",\n",
    "                                    \"application/xml;q=0.9,*/*;q=0.8\"]),\n",
    "               \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "                    \"User-Agent\":  ' '.join([\n",
    "                                    \"Mozilla/5.0]\",\n",
    "                                    \"(Macintosh; Intel Mac OS X 10_10_2)\",\n",
    "                                    \"AppleWebKit/600.3.18\",\n",
    "                                    \"(KHTML, like Gecko)\",\n",
    "                                    \"Version/8.0.3 Safari/600.3.18\"]),\n",
    "                 \"Cache-Control\": \"no-cache\"\n",
    "    }\n",
    "    queries = '&'.join(\n",
    "        [\n",
    "            '{key}={value}'.format(key=key, value=value)\n",
    "                for key, value in q.items()\n",
    "                if  key in inputs\n",
    "        ]\n",
    "    )\n",
    "    req = requests.get('?'.join([base, queries]), headers=headers)\n",
    "\n",
    "    if not req.ok:\n",
    "        print(\"Request didn't make it\", file=sys.stderr)\n",
    "        req.raise_for_status()\n",
    "\n",
    "    return req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def departments():\n",
    "    \"\"\"\n",
    "    Gets a mapping of all the deparments by key.\n",
    "    \"\"\"\n",
    "    logging.info('Grabbing a list of Departments')\n",
    "    prototype = connect(\"http\", department=\"CHEM\")\n",
    "    soup      = BeautifulSoup(prototype.content, 'lxml')\n",
    "    options   = list(reversed(soup.find_all('option')))\n",
    "\n",
    "    options.pop()\n",
    "\n",
    "    # Initial Course Mapping\n",
    "    mapping = dict(option.text.split(' - ') for option in options)\n",
    "\n",
    "    # Cleanup\n",
    "    for dept in ['BIOL', 'SOC', 'HIST', 'LING', 'LIT', 'NENG', 'RSM ', 'SOE', 'THEA']:\n",
    "        mapping.pop(dept)\n",
    "\n",
    "    # Actual Departments\n",
    "    mapping.update({\n",
    "        'BIBC': 'Biology Biochemistry',\n",
    "        'BILD': 'Biology Lower Division',\n",
    "        'BIMM': 'Biology Molecular, Microbiology',\n",
    "        'BIPN': 'Biology Physiology and Neuroscience',\n",
    "        'SOCA': 'Sociology Theory & Methods',\n",
    "        'SOCB': 'Sociology Cult, Lang, & Soc Interact',\n",
    "        'SOCC': 'Sociology Organiz & Institutions',\n",
    "        'SOCD': 'Sociology Comparative & Historical',\n",
    "        'SOCE': 'Sociology Ind Research & Honors Prog',\n",
    "        'SOCI': 'Sociology',\n",
    "        'SOCL': 'Sociology Lower Division',\n",
    "        'HILD': 'History Lower Division',\n",
    "        'HIAF': 'History of Africa',\n",
    "        'HIEA': 'History of East Asia',\n",
    "        'HIEU': 'History of Europe',\n",
    "        'HINE': 'History of Near East',\n",
    "        'HILA': 'History of Latin America',\n",
    "        'HISC': 'History of Science',\n",
    "        'HIUS': 'History of the United States',\n",
    "        'HITO': 'History Topics',\n",
    "        'LTAF': 'Literature African',\n",
    "        'LTAM': 'Literature of the Americas',\n",
    "        'LTCH': 'Literature Chinese',\n",
    "        'LTCS': 'Literature Cultural Studies',\n",
    "        'LTEA': 'Literature East Asian',\n",
    "        'LTEU': 'Literature European/Eurasian',\n",
    "        'LTFR': 'Literature French',\n",
    "        'LTGM': 'Literature General',\n",
    "        'LTGK': 'Literature Greek',\n",
    "        'LTGM': 'Literature German',\n",
    "        'LTIT': 'Literature Italian',\n",
    "        'LTKO': 'Literature Korean',\n",
    "        'LTLA': 'Literature Latin',\n",
    "        'LTRU': 'Literature Russian',\n",
    "        'LTSP': 'Literature Spanish',\n",
    "        'LTTH': 'Literature Theory',\n",
    "        'LTWL': 'Literature of the World',\n",
    "        'LTWR': 'Literature Writing',\n",
    "        'RELI': 'Literature Study of Religion',\n",
    "        'TWS' : 'Literature Third World Studies',\n",
    "        'NANO': 'Nano Engineering',\n",
    "        'MGT' : 'Rady School of Management',\n",
    "        'ENG' : 'Jacobs School of Engineering',\n",
    "        'LIGN': 'Linguistics',\n",
    "        'TDAC': 'Theatre Acting',\n",
    "        'TDCH': 'Theatre Dance Choreography',\n",
    "        'TDDE': 'Theatre Design',\n",
    "        'TDDR': 'Theatre Directing/Stage Management',\n",
    "        'TDGE': 'Theatre General',\n",
    "        'TDHD': 'Theatre Dance History',\n",
    "        'TDHT': 'Theatre History',\n",
    "        'TDMV': 'Theatre Dance Movement',\n",
    "        'TDPF': 'Theatre Dance Performance',\n",
    "        'TDPW': 'Theatre Playwriting',\n",
    "        'TDTR': 'Theatre Dance Theory',\n",
    "    })\n",
    "\n",
    "    # Create Categorical Series\n",
    "    dep = pd.Series(name='department_name', data=mapping)\n",
    "\n",
    "    # Reindexing\n",
    "    dep = dep.map(lambda x: np.nan if x == '' else x)\n",
    "    dep = dep.dropna()\n",
    "    dep.index.name = 'Departments'\n",
    "\n",
    "    return dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_percentage(element):\n",
    "    if isinstance(element, str):\n",
    "        return np.float(element.strip('%').strip()) / 100\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def calculate_grades(element):\n",
    "    if isinstance(element, str):\n",
    "        return np.float(element[1:].lstrip('+-').lstrip().strip('()'))\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def calculate_section_id(element):\n",
    "    if isinstance(element, str):\n",
    "        return int(element.lower().rsplit('sectionid=')[-1].strip(string.ascii_letters))\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def create_table(courses):\n",
    "    \"\"\"\n",
    "    Generates a pandas DataFrame by querying UCSD Cape Website.\n",
    "\n",
    "    Parameters\n",
    "    ==========\n",
    "    :params courses: Either Course or Path to HTML File\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "    :returns df:     Query Results\n",
    "    :rtype:          pandas.DataFrame\n",
    "    \"\"\"\n",
    "    header = [\n",
    "        'instructor', 'course', 'term', 'enroll', 'evals',\n",
    "        'recommend_class', 'recommend_instructor', 'study_hours_per_week',\n",
    "        'average_grade_expected', 'average_grade_received'\n",
    "    ]\n",
    "    first, second = itemgetter(0), itemgetter(1)\n",
    "\n",
    "    print('\\nGrabbing Classes: {0}'.format(courses))\n",
    "\n",
    "    # Get Data\n",
    "    base  = 'http://cape.ucsd.edu/responses/'\n",
    "    req   =  (\n",
    "                open(courses).read()\n",
    "                if   os.path.isfile(courses)\n",
    "                else connect(\"http\", courseNumber=courses).content\n",
    "            )\n",
    "    html  = BeautifulSoup(req, 'lxml')\n",
    "    table = first(html.find_all('table'))\n",
    "\n",
    "    # Create Dataframe\n",
    "    df    = first(pd.read_html(str(table)), flavor=None, na_values=['No CAPEs submitted'])\n",
    "\n",
    "    # Data Clean Up\n",
    "    df.columns = header\n",
    "    df['link']       = [\n",
    "        urljoin(base, link.attrs['href']) if link.has_attr('href') else np.nan\n",
    "            for link in table.find_all('a')\n",
    "    ]\n",
    "    df['instructor'] = df.instructor.map(\n",
    "        lambda name: (\n",
    "            str.title(name)\n",
    "            if isinstance(name, str) else 'Unknown, Unknonwn'\n",
    "        )\n",
    "    )\n",
    "    # Data Extraction\n",
    "    df['first_name']  = df.instructor.map(lambda name:  second(name.split(',')).strip('.'))\n",
    "    df['last_name']   = df.instructor.map(lambda name:   first(name.split(',')))\n",
    "    df['class_id']    = df.course.map(  lambda course: first(course.split(' - ')))\n",
    "    df['department']  = df.class_id.map(lambda course:  first(course.split(' ')))\n",
    "    df['class_name']  = df.course.map(\n",
    "        lambda course: (\n",
    "            second(course.split(' - '))[:-4]\n",
    "            if ' - ' in course else np.nan)\n",
    "    )\n",
    "    # Data Types\n",
    "    df['recommend_class']        = df.recommend_class.map(calculate_percentage)\n",
    "    df['recommend_instructor']   = df.recommend_instructor.map(calculate_percentage)\n",
    "    df['average_grade_expected'] = df.average_grade_expected.map(calculate_grades)\n",
    "    df['average_grade_received'] = df.average_grade_received.map(calculate_grades)\n",
    "\n",
    "    # Reindexing and Transforms\n",
    "    df['section_id'] = df.link.map(calculate_section_id)\n",
    "    df = df.dropna(subset=['section_id'])\n",
    "    df = df.drop_duplicates(subset='section_id')\n",
    "    df['section_id'] = df.section_id.astype(np.int32)\n",
    "\n",
    "    return df.set_index('section_id', drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def to_db(df, table, user='postgres', db='graphucsd', resolve='replace'):\n",
    "    \"\"\"\n",
    "    Helper Function to Push DataFrame to Postgresql Database\n",
    "    \"\"\"\n",
    "    url = 'postgresql+psycopg2://{user}@localhost/{db}'.format(user=user, db=db)\n",
    "\n",
    "    if not database_exists(url):\n",
    "        create_database(url)\n",
    "\n",
    "    engine = create_engine(url)\n",
    "\n",
    "    return df.to_sql(table, engine, if_exists=resolve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def main(threads=6):\n",
    "    \"\"\"\n",
    "    Get all departments\n",
    "    \"\"\"\n",
    "    logging.info('Program is Starting')\n",
    "\n",
    "    # Get Departments\n",
    "    deps  = departments()\n",
    "    keys  = [department.strip() for department in deps.keys()]\n",
    "\n",
    "    # Run Scraper Concurrently Using ThreadPool\n",
    "    pool  = ThreadPool(threads)\n",
    "    logging.info('Initialize Scraper with {} Threads'.format(threads))\n",
    "    table = pool.map(create_table, keys)\n",
    "    logging.info('Scrape Complete')\n",
    "\n",
    "    # Manage ThreadPool\n",
    "    pool.close(); pool.join()\n",
    "\n",
    "    df = pd.concat(table)\n",
    "\n",
    "    return df.groupby(level=0).first()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
